{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37628c8e-794d-4bc1-ab3b-f86b95f5a6b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ajinnuri/Downloads/celiac_disease_lab_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"/Users/ajinnuri/Downloads/celiac_disease_lab_data.csv\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "df = df.drop(columns=['cd_type'])\n",
    "df= df.drop(columns=['Diabetes'])\n",
    "df=df.drop(columns=['Diabetes Type'])\n",
    "df=df.drop(columns=['Gender'])\n",
    "df=df.drop(columns=['Marsh'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf3b7ba-8b6b-435a-b795-0f53efbd17e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping NaN rows:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore dropping NaN rows:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mdropna(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter dropping NaN rows:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping NaN rows:\")\n",
    "print(df.head())\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"After dropping NaN rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e33ecbb0-0bdd-4f87-a17f-aae285f67c39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiarrhoea\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiarrhoea\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcodes\n\u001b[1;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbdominal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbdominal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcodes\n\u001b[1;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShort_Stature\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShort_Stature\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcodes\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['Diarrhoea']=df['Diarrhoea'].astype('category').cat.codes\n",
    "df['Abdominal']=df['Abdominal'].astype('category').cat.codes\n",
    "df['Short_Stature']=df['Short_Stature'].astype('category').cat.codes\n",
    "df['Sticky_Stool']=df['Sticky_Stool'].astype('category').cat.codes\n",
    "df['Weight_loss']=df['Weight_loss'].astype('category').cat.codes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f167b20-232d-4a1d-bed0-e4baa3cb4016",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m13\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m sns\u001b[38;5;241m.\u001b[39mhistplot(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m],kde\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "plt.subplot(1,4,1)\n",
    "sns.histplot(df['Age'],kde=True,bins=20)\n",
    "plt.title('Age distribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbc5dc4f-8e91-4003-abb2-ea1fcb646629",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m13\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m sns\u001b[38;5;241m.\u001b[39mhistplot(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiarrhoea\u001b[39m\u001b[38;5;124m'\u001b[39m],kde\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "plt.subplot(1,4,1)\n",
    "sns.histplot(df['Diarrhoea'],kde=True,bins=20)\n",
    "plt.title('Diarrhoea Distribution')\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "sns.histplot(df['Abdominal'],kde=True, bins=20)\n",
    "plt.title('Abdominal Distribution')\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "sns.histplot(df['Short_Stature'],kde=True,bins=20)\n",
    "plt.title('Short Stature Distribution')\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "sns.histplot(df['Sticky_Stool'], kde=True, bins=20)\n",
    "plt.title('Sticky Stool Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e005969-854b-4cb0-bf71-53d19aaff1c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m13\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m sns\u001b[38;5;241m.\u001b[39mhistplot(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeight_loss\u001b[39m\u001b[38;5;124m'\u001b[39m],kde\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "plt.subplot(1,4,1)\n",
    "sns.histplot(df['Weight_loss'],kde=True,bins=20)\n",
    "plt.title('Weight Loss Distribution')\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "sns.histplot(df['IgA'],kde=True, bins=20)\n",
    "plt.title('IgA Distribution')\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "sns.histplot(df['IgG'],kde=True,bins=20)\n",
    "plt.title('IgG Distribution')\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "sns.histplot(df['IgM'], kde=True, bins=20)\n",
    "plt.title('IgM Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dc51f61-ebe1-402c-b4ab-8b38d93f843c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisease_Diagnose\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisease_Diagnose\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "X = df.drop('Disease_Diagnose', axis=1)\n",
    "y = df['Disease_Diagnose']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "y_pred_lr = logistic_regression.predict(X_test)\n",
    "print(\"Logistic Regression Classifier\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr) * 100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c41bbdd-227c-4190-824e-72aa539af3f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define hyperparameters to tune\u001b[39;00m\n\u001b[1;32m      4\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[1;32m     10\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=2, n_jobs=-1, verbose=3)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Accuracy of the best model: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90b4dc5b-9597-49aa-9ccc-94781fb941ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m\n\u001b[1;32m      1\u001b[0m param_grids \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVC\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     }\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize models\u001b[39;00m\n\u001b[1;32m     22\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVC\u001b[39m\u001b[38;5;124m'\u001b[39m: SVC(),\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: RandomForestClassifier(),\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     26\u001b[0m }\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Perform Grid Search for each model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SVC' is not defined"
     ]
    }
   ],
   "source": [
    "param_grids = {\n",
    "    'SVC': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['lbfgs', 'liblinear'],\n",
    "        'penalty': ['l2']  # 'l1' can be used if solver='liblinear'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'SVC': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Perform Grid Search for each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Performing Grid Search for {name}...\")\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[name], \n",
    "                               cv=20, n_jobs=-1, verbose=2)\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best parameters and model\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions and evaluate the model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f'Best Parameters for {name}: {best_params}')\n",
    "    print(f'Accuracy of the best model for {name}: {accuracy:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b3bbb73-70f8-4008-9a03-8375926b8e2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Gradient Boosting Classifier\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Standardize the data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Gradient Boosting classifier\n",
    "gradient_boosting = GradientBoostingClassifier()\n",
    "gradient_boosting.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gradient_boosting.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = np.unique(y)  # Get class names from the dataset\n",
    "plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "# Display precision, recall, and F1 scores\n",
    "print(\"Precision, Recall, and F1 Scores:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[str(cls) for cls in class_names]))\n",
    "\n",
    "# Feature importance\n",
    "importances = gradient_boosting.feature_importances_\n",
    "feature_names = X.columns  # Assuming your feature names are the columns of X\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importance from Gradient Boosting')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65dafe98-aaa1-4d08-82f8-444766c1145c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisease_Diagnose\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisease_Diagnose\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "X = df.drop('Disease_Diagnose', axis=1)\n",
    "y = df['Disease_Diagnose']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = np.unique(y)  # Get class names from the dataset\n",
    "plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "# Display precision, recall, and F1 scores\n",
    "print(\"Precision, Recall, and F1 Scores:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[str(cls) for cls in class_names]))\n",
    "\n",
    "# Feature importance\n",
    "importances = random_forest.feature_importances_\n",
    "feature_names = X.columns  # Assuming your feature names are the columns of X\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47ea3734-43ad-436d-ac71-4603fa05a05e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_forest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m importances \u001b[38;5;241m=\u001b[39m random_forest\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[1;32m      2\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Assuming the last column is the label\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for better visualization\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_forest' is not defined"
     ]
    }
   ],
   "source": [
    "importances = random_forest.feature_importances_\n",
    "feature_names = df.columns[:-1]  # Assuming the last column is the label\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d998b00-8d39-48b7-b1d0-0cd8b1850419",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisease_Diagnose\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisease_Diagnose\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "X = df.drop('Disease_Diagnose', axis=1)\n",
    "y = df['Disease_Diagnose']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the SVC model\n",
    "svc = SVC(kernel='linear', random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = np.unique(y)  # Get class names from the dataset\n",
    "plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "# Display precision, recall, and F1 scores\n",
    "print(\"Precision, Recall, and F1 Scores:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[str(cls) for cls in class_names]))\n",
    "\n",
    "# Feature importance for linear kernel SVC\n",
    "importances = np.abs(svc.coef_[0])\n",
    "feature_names = X.columns  # Assuming your feature names are the columns of X\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importance from SVC')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c5290321-44ca-4114-bac2-1642a1cb783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector Classifier: 0.92\n",
      "Accuracy of Logistic Regression: 0.90\n",
      "Accuracy of Gradient Boosting: 0.95\n",
      "Accuracy of Random Forest: 0.95\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Support Vector Classifier': SVC(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy of {name}: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "648bf363-43ce-43b9-835a-15b1d92223ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for the single instance: no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def predict_single_instance(model, scaler, instance):\n",
    "    # Convert the instance to a 2D array\n",
    "    instance_array = np.array(instance).reshape(1, -1)\n",
    "    \n",
    "    # Standardize the instance\n",
    "    instance_scaled = scaler.transform(instance_array)\n",
    "    \n",
    "    # Make the prediction\n",
    "    prediction = model.predict(instance_scaled)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Example single instance (replace with actual feature values)\n",
    "single_instance = [10,1,0,0,1,1,1,1.00,0.3]  # Example from the Iris dataset\n",
    "\n",
    "# Predict the class of the single instance\n",
    "predicted_class = predict_single_instance(random_forest, scaler, single_instance)\n",
    "print(f'Predicted class for the single instance: {predicted_class[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c124576-64a3-4a62-aac9-a230e88ee267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_instance(model, scaler, instance):\n",
    "    # Convert the instance to a 2D array\n",
    "    instance_array = np.array(instance).reshape(1, -1)\n",
    "    \n",
    "    # Standardize the instance\n",
    "    instance_scaled = scaler.transform(instance_array)\n",
    "    \n",
    "    # Make the prediction\n",
    "    prediction = model.predict(instance_scaled)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Example single instance (replace with actual feature values)\n",
    "single_instance = [10,1,0,0,1,1,1,0,0,1.3,10.0,1.00,0.3]  # Example from the Iris dataset\n",
    "\n",
    "# Predict the class of the single instance\n",
    "predicted_class = predict_single_instance(gradient_boosting, scaler, single_instance)\n",
    "print(f'Predicted class for the single instance: {predicted_class[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762fbb86-fbdf-4dc9-9f54-9f0c406affbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a792733a-c259-443b-ae1c-d36065f17c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
